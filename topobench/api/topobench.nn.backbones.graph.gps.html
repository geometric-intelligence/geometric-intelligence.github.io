
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>topobench.nn.backbones.graph.gps module &#8212; TopoBench latest documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=bafc5e63" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=c6e86fd7"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'api/topobench.nn.backbones.graph.gps';</script>
    <link rel="canonical" href="https://geometric-intelligence.github.io/topobench/api/topobench.nn.backbones.graph.gps.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="topobench.nn.backbones.graph.graph_mlp module" href="topobench.nn.backbones.graph.graph_mlp.html" />
    <link rel="prev" title="topobench.nn.backbones.graph.nsd_utils.sheaf_models module" href="topobench.nn.backbones.graph.nsd_utils.sheaf_models.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="latest" />
    <meta name="docbuild:last-update" content="Oct 29, 2025, 11:42:18 PM"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">TopoBench latest documentation</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../tdl-challenge/index.html">
    TAG-DS TDL Challenge 2025
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    API Reference
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../contributing/index.html">
    Contributing
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../tdl-challenge/index.html">
    TAG-DS TDL Challenge 2025
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    API Reference
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../contributing/index.html">
    Contributing
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">API Reference</a></li>
    
    
    <li class="breadcrumb-item"><a href="topobench.html" class="nav-link">topobench package</a></li>
    
    
    <li class="breadcrumb-item"><a href="topobench.nn.html" class="nav-link">topobench.nn package</a></li>
    
    
    <li class="breadcrumb-item"><a href="topobench.nn.backbones.html" class="nav-link">topobench.nn.backbones package</a></li>
    
    
    <li class="breadcrumb-item"><a href="topobench.nn.backbones.graph.html" class="nav-link">topobench.nn.backbones.graph package</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">topobench.nn.backbones.graph.gps module</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="module-topobench.nn.backbones.graph.gps">
<span id="topobench-nn-backbones-graph-gps-module"></span><h1>topobench.nn.backbones.graph.gps module<a class="headerlink" href="#module-topobench.nn.backbones.graph.gps" title="Link to this heading">#</a></h1>
<p>This module implements a GPS-based model[1] that can be used with the training framework.</p>
<p>GPS combines local message passing with global attention mechanisms.
Uses the official PyTorch Geometric GPSConv implementation.</p>
<p>[1] Rampášek, Ladislav, et al. “Recipe for a general, powerful, scalable graph transformer.”
Advances in Neural Information Processing Systems 35 (2022): 14501-14515.</p>
<dl class="py class">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.Any">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">Any</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.Any" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Special type indicating an unconstrained type.</p>
<ul class="simple">
<li><p>Any is compatible with every type.</p></li>
<li><p>Any assumed to have all methods.</p></li>
<li><p>All values assumed to be instances of Any.</p></li>
</ul>
<p>Note that all the above statements are true from the point of view of
static type checkers. At runtime, Any should not be used with instance
checks.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.GINConv">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">GINConv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">nn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.GINConv" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">MessagePassing</span></code></p>
<p>The graph isomorphism operator from the <a class="reference external" href="https://arxiv.org/abs/1810.00826">“How Powerful are
Graph Neural Networks?”</a> paper.</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}^{\prime}_i = h_{\mathbf{\Theta}} \left( (1 + \epsilon) \cdot
\mathbf{x}_i + \sum_{j \in \mathcal{N}(i)} \mathbf{x}_j \right)\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[\mathbf{X}^{\prime} = h_{\mathbf{\Theta}} \left( \left( \mathbf{A} +
(1 + \epsilon) \cdot \mathbf{I} \right) \cdot \mathbf{X} \right),\]</div>
<p>here <span class="math notranslate nohighlight">\(h_{\mathbf{\Theta}}\)</span> denotes a neural network, <em>.i.e.</em> an MLP.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>nn</strong> (<em>torch.nn.Module</em>) – A neural network <span class="math notranslate nohighlight">\(h_{\mathbf{\Theta}}\)</span> that
maps node features <code class="xref py py-obj docutils literal notranslate"><span class="pre">x</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">[-1,</span> <span class="pre">in_channels]</span></code> to
shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">[-1,</span> <span class="pre">out_channels]</span></code>, <em>e.g.</em>, defined by
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Sequential</span></code>.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – (Initial) <span class="math notranslate nohighlight">\(\epsilon\)</span>-value.
(default: <code class="xref py py-obj docutils literal notranslate"><span class="pre">0.</span></code>)</p></li>
<li><p><strong>train_eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>) – If set to <a class="reference external" href="https://docs.python.org/3/library/constants.html#True" title="(in Python v3.14)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code></a>, <span class="math notranslate nohighlight">\(\epsilon\)</span>
will be a trainable parameter. (default: <a class="reference external" href="https://docs.python.org/3/library/constants.html#False" title="(in Python v3.14)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code></a>)</p></li>
<li><p><strong>**kwargs</strong> (<em>optional</em>) – Additional arguments of
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch_geometric.nn.conv.MessagePassing</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shapes:</dt><dd><ul class="simple">
<li><p><strong>input:</strong>
node features <span class="math notranslate nohighlight">\((|\mathcal{V}|, F_{in})\)</span> or
<span class="math notranslate nohighlight">\(((|\mathcal{V_s}|, F_{s}), (|\mathcal{V_t}|, F_{t}))\)</span>
if bipartite,
edge indices <span class="math notranslate nohighlight">\((2, |\mathcal{E}|)\)</span></p></li>
<li><p><strong>output:</strong> node features <span class="math notranslate nohighlight">\((|\mathcal{V}|, F_{out})\)</span> or
<span class="math notranslate nohighlight">\((|\mathcal{V}_t|, F_{out})\)</span> if bipartite</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.GINConv.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">nn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.GINConv.__init__" title="Link to this definition">#</a></dt>
<dd><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.GINConv.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.GINConv.forward" title="Link to this definition">#</a></dt>
<dd><p>Runs the forward pass of the module.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.GINConv.message">
<span class="sig-name descname"><span class="pre">message</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_j</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.GINConv.message" title="Link to this definition">#</a></dt>
<dd><p>Constructs messages from node <span class="math notranslate nohighlight">\(j\)</span> to node <span class="math notranslate nohighlight">\(i\)</span>
in analogy to <span class="math notranslate nohighlight">\(\phi_{\mathbf{\Theta}}\)</span> for each edge in
<code class="xref py py-obj docutils literal notranslate"><span class="pre">edge_index</span></code>.
This function can take any argument as input which was initially
passed to <code class="xref py py-meth docutils literal notranslate"><span class="pre">propagate()</span></code>.
Furthermore, tensors passed to <code class="xref py py-meth docutils literal notranslate"><span class="pre">propagate()</span></code> can be mapped to the
respective nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> by appending <code class="xref py py-obj docutils literal notranslate"><span class="pre">_i</span></code> or
<code class="xref py py-obj docutils literal notranslate"><span class="pre">_j</span></code> to the variable name, <em>.e.g.</em> <code class="xref py py-obj docutils literal notranslate"><span class="pre">x_i</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">x_j</span></code>.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.GINConv.message_and_aggregate">
<span class="sig-name descname"><span class="pre">message_and_aggregate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">adj_t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.GINConv.message_and_aggregate" title="Link to this definition">#</a></dt>
<dd><p>Fuses computations of <a class="reference internal" href="#topobench.nn.backbones.graph.gps.GINConv.message" title="topobench.nn.backbones.graph.gps.GINConv.message"><code class="xref py py-func docutils literal notranslate"><span class="pre">message()</span></code></a> and <code class="xref py py-func docutils literal notranslate"><span class="pre">aggregate()</span></code> into a
single function.
If applicable, this saves both time and memory since messages do not
explicitly need to be materialized.
This function will only gets called in case it is implemented and
propagation takes place based on a <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch_sparse.SparseTensor</span></code>
or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.sparse.Tensor</span></code>.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.GINConv.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.GINConv.reset_parameters" title="Link to this definition">#</a></dt>
<dd><p>Resets all learnable parameters of the module.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.GPSConv">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">GPSConv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'batch_norm'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'multihead'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.GPSConv" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="topobench.nn.backbones.graph.nsd.html#topobench.nn.backbones.graph.nsd.Module" title="torch.nn.modules.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>The general, powerful, scalable (GPS) graph transformer layer from the
<a class="reference external" href="https://arxiv.org/abs/2205.12454">“Recipe for a General, Powerful, Scalable Graph Transformer”</a> paper.</p>
<p>The GPS layer is based on a 3-part recipe:</p>
<ol class="arabic simple">
<li><p>Inclusion of positional (PE) and structural encodings (SE) to the input
features (done in a pre-processing step via
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch_geometric.transforms</span></code>).</p></li>
<li><p>A local message passing layer (MPNN) that operates on the input graph.</p></li>
<li><p>A global attention layer that operates on the entire graph.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For an example of using <a class="reference internal" href="#topobench.nn.backbones.graph.gps.GPSConv" title="topobench.nn.backbones.graph.gps.GPSConv"><code class="xref py py-class docutils literal notranslate"><span class="pre">GPSConv</span></code></a>, see
<a class="reference external" href="https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_gps.py">examples/graph_gps.py</a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a>) – Size of each input sample.</p></li>
<li><p><strong>conv</strong> (<em>MessagePassing</em><em>, </em><em>optional</em>) – The local message passing layer.</p></li>
<li><p><strong>heads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a><em>, </em><em>optional</em>) – Number of multi-head-attentions.
(default: <code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code>)</p></li>
<li><p><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout probability of intermediate
embeddings. (default: <code class="xref py py-obj docutils literal notranslate"><span class="pre">0.</span></code>)</p></li>
<li><p><strong>act</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em> or </em><a class="reference internal" href="topobench.utils.utils.html#topobench.utils.utils.Callable" title="topobench.utils.utils.Callable"><em>Callable</em></a><em>, </em><em>optional</em>) – The non-linear activation function to
use. (default: <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;relu&quot;</span></code>)</p></li>
<li><p><strong>act_kwargs</strong> (<em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>, </em><a class="reference internal" href="#topobench.nn.backbones.graph.gps.Any" title="topobench.nn.backbones.graph.gps.Any"><em>Any</em></a><em>]</em><em>, </em><em>optional</em>) – Arguments passed to the
respective activation function defined by <code class="xref py py-obj docutils literal notranslate"><span class="pre">act</span></code>.
(default: <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a>)</p></li>
<li><p><strong>norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em> or </em><a class="reference internal" href="topobench.utils.utils.html#topobench.utils.utils.Callable" title="topobench.utils.utils.Callable"><em>Callable</em></a><em>, </em><em>optional</em>) – The normalization function to
use. (default: <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;batch_norm&quot;</span></code>)</p></li>
<li><p><strong>norm_kwargs</strong> (<em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>, </em><a class="reference internal" href="#topobench.nn.backbones.graph.gps.Any" title="topobench.nn.backbones.graph.gps.Any"><em>Any</em></a><em>]</em><em>, </em><em>optional</em>) – Arguments passed to the
respective normalization function defined by <code class="xref py py-obj docutils literal notranslate"><span class="pre">norm</span></code>.
(default: <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a>)</p></li>
<li><p><strong>attn_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>) – Global attention type, <code class="xref py py-obj docutils literal notranslate"><span class="pre">multihead</span></code> or
<code class="xref py py-obj docutils literal notranslate"><span class="pre">performer</span></code>. (default: <code class="xref py py-obj docutils literal notranslate"><span class="pre">multihead</span></code>)</p></li>
<li><p><strong>attn_kwargs</strong> (<em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>, </em><a class="reference internal" href="#topobench.nn.backbones.graph.gps.Any" title="topobench.nn.backbones.graph.gps.Any"><em>Any</em></a><em>]</em><em>, </em><em>optional</em>) – Arguments passed to the
attention layer. (default: <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a>)</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.GPSConv.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'batch_norm'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'multihead'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.GPSConv.__init__" title="Link to this definition">#</a></dt>
<dd><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.GPSConv.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.GPSConv.forward" title="Link to this definition">#</a></dt>
<dd><p>Runs the forward pass of the module.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.GPSConv.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.GPSConv.reset_parameters" title="Link to this definition">#</a></dt>
<dd><p>Resets all learnable parameters of the module.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.GPSEncoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">GPSEncoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'multihead'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_conv_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gin'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_edge_attr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">redraw_interval</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.GPSEncoder" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="topobench.nn.backbones.graph.nsd.html#topobench.nn.backbones.graph.nsd.Module" title="torch.nn.modules.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>GPS Encoder that can be used with the training framework.</p>
<p>Uses the official PyTorch Geometric GPSConv implementation.
This encoder combines local message passing with global attention mechanisms
for powerful graph representation learning.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>input_dim</strong><span class="classifier">int</span></dt><dd><p>Dimension of input node features.</p>
</dd>
<dt><strong>hidden_dim</strong><span class="classifier">int</span></dt><dd><p>Dimension of hidden layers.</p>
</dd>
<dt><strong>num_layers</strong><span class="classifier">int, optional</span></dt><dd><p>Number of GPS layers. Default is 4.</p>
</dd>
<dt><strong>heads</strong><span class="classifier">int, optional</span></dt><dd><p>Number of attention heads in GPSConv layers. Default is 4.</p>
</dd>
<dt><strong>dropout</strong><span class="classifier">float, optional</span></dt><dd><p>Dropout rate for GPSConv layers. Default is 0.1.</p>
</dd>
<dt><strong>attn_type</strong><span class="classifier">str, optional</span></dt><dd><p>Type of attention mechanism to use. Options are ‘multihead’, ‘performer’, etc.
Default is ‘multihead’.</p>
</dd>
<dt><strong>local_conv_type</strong><span class="classifier">str, optional</span></dt><dd><p>Type of local message passing layer. Options are ‘gin’, ‘pna’, etc.
Default is ‘gin’.</p>
</dd>
<dt><strong>use_edge_attr</strong><span class="classifier">bool, optional</span></dt><dd><p>Whether to use edge attributes in GPSConv layers. Default is False.</p>
</dd>
<dt><strong>redraw_interval</strong><span class="classifier">int or None, optional</span></dt><dd><p>Interval for redrawing random projections in Performer attention.
If None, projections are not redrawn. Default is None.</p>
</dd>
<dt><strong>attn_kwargs</strong><span class="classifier">dict, optional</span></dt><dd><p>Additional keyword arguments for the attention mechanism.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.GPSEncoder.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'multihead'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_conv_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gin'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_edge_attr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">redraw_interval</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.GPSEncoder.__init__" title="Link to this definition">#</a></dt>
<dd><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.GPSEncoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_attr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.GPSEncoder.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass of GPS encoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong><span class="classifier">torch.Tensor</span></dt><dd><p>Node feature matrix of shape [num_nodes, input_dim].</p>
</dd>
<dt><strong>edge_index</strong><span class="classifier">torch.Tensor</span></dt><dd><p>Edge indices of shape [2, num_edges].</p>
</dd>
<dt><strong>batch</strong><span class="classifier">torch.Tensor, optional</span></dt><dd><p>Batch vector assigning each node to a specific graph. Shape [num_nodes]. Default is None.</p>
</dd>
<dt><strong>edge_attr</strong><span class="classifier">torch.Tensor, optional</span></dt><dd><p>Edge feature matrix of shape [num_edges, edge_dim]. Default is None.</p>
</dd>
<dt><strong>**kwargs</strong><span class="classifier">dict</span></dt><dd><p>Additional arguments (not used).</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>torch.Tensor</dt><dd><p>Output node feature matrix of shape [num_nodes, hidden_dim].</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.PNAConv">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">PNAConv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aggregators</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scalers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">deg</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">towers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">post_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">divide_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.PNAConv" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">MessagePassing</span></code></p>
<p>The Principal Neighbourhood Aggregation graph convolution operator
from the <a class="reference external" href="https://arxiv.org/abs/2004.05718">“Principal Neighbourhood Aggregation for Graph Nets”</a> paper.</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}_i^{\prime} = \gamma_{\mathbf{\Theta}} \left(
\mathbf{x}_i, \underset{j \in \mathcal{N}(i)}{\bigoplus}
h_{\mathbf{\Theta}} \left( \mathbf{x}_i, \mathbf{x}_j \right)
\right)\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[\begin{split}\bigoplus = \underbrace{\begin{bmatrix}
    1 \\
    S(\mathbf{D}, \alpha=1) \\
    S(\mathbf{D}, \alpha=-1)
\end{bmatrix} }_{\text{scalers}}
\otimes \underbrace{\begin{bmatrix}
    \mu \\
    \sigma \\
    \max \\
    \min
\end{bmatrix}}_{\text{aggregators}},\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma_{\mathbf{\Theta}}\)</span> and <span class="math notranslate nohighlight">\(h_{\mathbf{\Theta}}\)</span>
denote MLPs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For an example of using <a class="reference internal" href="#topobench.nn.backbones.graph.gps.PNAConv" title="topobench.nn.backbones.graph.gps.PNAConv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PNAConv</span></code></a>, see <a class="reference external" href="https://github.com/pyg-team/pytorch_geometric/blob/master/examples/pna.py">examples/pna.py</a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a>) – Size of each input sample, or <code class="xref py py-obj docutils literal notranslate"><span class="pre">-1</span></code> to derive
the size from the first input(s) to the forward method.</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a>) – Size of each output sample.</p></li>
<li><p><strong>aggregators</strong> (<em>List</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>]</em>) – Set of aggregation function identifiers,
namely <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;min&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code>,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;var&quot;</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;std&quot;</span></code>.</p></li>
<li><p><strong>scalers</strong> (<em>List</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>]</em>) – Set of scaling function identifiers, namely
<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;identity&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;amplification&quot;</span></code>,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;attenuation&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;linear&quot;</span></code> and
<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;inverse_linear&quot;</span></code>.</p></li>
<li><p><strong>deg</strong> (<em>torch.Tensor</em>) – Histogram of in-degrees of nodes in the training
set, used by scalers to normalize.</p></li>
<li><p><strong>edge_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a><em>, </em><em>optional</em>) – Edge feature dimensionality (in case
there are any). (default <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a>)</p></li>
<li><p><strong>towers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a><em>, </em><em>optional</em>) – Number of towers (default: <code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code>).</p></li>
<li><p><strong>pre_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a><em>, </em><em>optional</em>) – Number of transformation layers before
aggregation (default: <code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code>).</p></li>
<li><p><strong>post_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a><em>, </em><em>optional</em>) – Number of transformation layers after
aggregation (default: <code class="xref py py-obj docutils literal notranslate"><span class="pre">1</span></code>).</p></li>
<li><p><strong>divide_input</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether the input features should
be split between towers or not (default: <a class="reference external" href="https://docs.python.org/3/library/constants.html#False" title="(in Python v3.14)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code></a>).</p></li>
<li><p><strong>act</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em> or </em><em>callable</em><em>, </em><em>optional</em>) – Pre- and post-layer activation
function to use. (default: <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;relu&quot;</span></code>)</p></li>
<li><p><strong>act_kwargs</strong> (<em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>, </em><a class="reference internal" href="#topobench.nn.backbones.graph.gps.Any" title="topobench.nn.backbones.graph.gps.Any"><em>Any</em></a><em>]</em><em>, </em><em>optional</em>) – Arguments passed to the
respective activation function defined by <code class="xref py py-obj docutils literal notranslate"><span class="pre">act</span></code>.
(default: <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a>)</p></li>
<li><p><strong>train_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether normalization parameters
are trainable. (default: <a class="reference external" href="https://docs.python.org/3/library/constants.html#False" title="(in Python v3.14)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code></a>)</p></li>
<li><p><strong>**kwargs</strong> (<em>optional</em>) – Additional arguments of
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch_geometric.nn.conv.MessagePassing</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shapes:</dt><dd><ul class="simple">
<li><p><strong>input:</strong>
node features <span class="math notranslate nohighlight">\((|\mathcal{V}|, F_{in})\)</span>,
edge indices <span class="math notranslate nohighlight">\((2, |\mathcal{E}|)\)</span>,
edge features <span class="math notranslate nohighlight">\((|\mathcal{E}|, D)\)</span> <em>(optional)</em></p></li>
<li><p><strong>output:</strong> node features <span class="math notranslate nohighlight">\((|\mathcal{V}|, F_{out})\)</span></p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.PNAConv.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aggregators</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scalers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">deg</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">towers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">post_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">divide_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.PNAConv.__init__" title="Link to this definition">#</a></dt>
<dd><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.PNAConv.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_attr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.PNAConv.forward" title="Link to this definition">#</a></dt>
<dd><p>Runs the forward pass of the module.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.PNAConv.get_degree_histogram">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">get_degree_histogram</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loader</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.PNAConv.get_degree_histogram" title="Link to this definition">#</a></dt>
<dd><p>Returns the degree histogram to be used as input for the <code class="xref py py-obj docutils literal notranslate"><span class="pre">deg</span></code>
argument in <a class="reference internal" href="#topobench.nn.backbones.graph.gps.PNAConv" title="topobench.nn.backbones.graph.gps.PNAConv"><code class="xref py py-class docutils literal notranslate"><span class="pre">PNAConv</span></code></a>.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.PNAConv.message">
<span class="sig-name descname"><span class="pre">message</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_i</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x_j</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_attr</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.PNAConv.message" title="Link to this definition">#</a></dt>
<dd><p>Constructs messages from node <span class="math notranslate nohighlight">\(j\)</span> to node <span class="math notranslate nohighlight">\(i\)</span>
in analogy to <span class="math notranslate nohighlight">\(\phi_{\mathbf{\Theta}}\)</span> for each edge in
<code class="xref py py-obj docutils literal notranslate"><span class="pre">edge_index</span></code>.
This function can take any argument as input which was initially
passed to <code class="xref py py-meth docutils literal notranslate"><span class="pre">propagate()</span></code>.
Furthermore, tensors passed to <code class="xref py py-meth docutils literal notranslate"><span class="pre">propagate()</span></code> can be mapped to the
respective nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> by appending <code class="xref py py-obj docutils literal notranslate"><span class="pre">_i</span></code> or
<code class="xref py py-obj docutils literal notranslate"><span class="pre">_j</span></code> to the variable name, <em>.e.g.</em> <code class="xref py py-obj docutils literal notranslate"><span class="pre">x_i</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">x_j</span></code>.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.PNAConv.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.PNAConv.reset_parameters" title="Link to this definition">#</a></dt>
<dd><p>Resets all learnable parameters of the module.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.PerformerAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">PerformerAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">ReLU()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qkv_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_out_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.PerformerAttention" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="topobench.nn.backbones.graph.nsd.html#topobench.nn.backbones.graph.nsd.Module" title="torch.nn.modules.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>The linear scaled attention mechanism from the
<a class="reference external" href="https://arxiv.org/abs/2009.14794">“Rethinking Attention with Performers”</a> paper.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a>) – Size of each input sample.</p></li>
<li><p><strong>heads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a><em>, </em><em>optional</em>) – Number of parallel attention heads.</p></li>
<li><p><strong>head_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a><em>, </em><em>optional</em>) – Size of each attention head.
(default: <code class="xref py py-obj docutils literal notranslate"><span class="pre">64.</span></code>)</p></li>
<li><p><strong>kernel</strong> (<a class="reference internal" href="topobench.utils.utils.html#topobench.utils.utils.Callable" title="topobench.utils.utils.Callable"><em>Callable</em></a><em>, </em><em>optional</em>) – Kernels for generalized attention.
If not specified, <cite>ReLU</cite> kernel will be used.
(default: <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.ReLU()</span></code>)</p></li>
<li><p><strong>qkv_bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>) – If specified, add bias to query, key
and value in the self attention. (default: <a class="reference external" href="https://docs.python.org/3/library/constants.html#False" title="(in Python v3.14)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code></a>)</p></li>
<li><p><strong>attn_out_bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>) – If specified, add bias to the
attention output. (default: <a class="reference external" href="https://docs.python.org/3/library/constants.html#True" title="(in Python v3.14)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">True</span></code></a>)</p></li>
<li><p><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout probability of the final
attention output. (default: <code class="xref py py-obj docutils literal notranslate"><span class="pre">0.0</span></code>)</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.PerformerAttention.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">ReLU()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qkv_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_out_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.PerformerAttention.__init__" title="Link to this definition">#</a></dt>
<dd><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.PerformerAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.PerformerAttention.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – Node feature tensor
<span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{B \times N \times F}\)</span>, with
batch-size <span class="math notranslate nohighlight">\(B\)</span>, (maximum) number of nodes <span class="math notranslate nohighlight">\(N\)</span> for
each graph, and feature dimension <span class="math notranslate nohighlight">\(F\)</span>.</p></li>
<li><p><strong>mask</strong> (<em>torch.Tensor</em><em>, </em><em>optional</em>) – Mask matrix
<span class="math notranslate nohighlight">\(\mathbf{M} \in {\{ 0, 1 \}}^{B \times N}\)</span> indicating
the valid nodes for each graph. (default: <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a>)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.PerformerAttention.redraw_projection_matrix">
<span class="sig-name descname"><span class="pre">redraw_projection_matrix</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.PerformerAttention.redraw_projection_matrix" title="Link to this definition">#</a></dt>
<dd><p>As described in the paper, periodically redraw
examples to improve overall approximation of attention.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.RedrawProjection">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">RedrawProjection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">redraw_interval</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.RedrawProjection" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Helper class to handle redrawing of random projections in Performer attention.</p>
<p>This is crucial for maintaining the quality of the random feature approximation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>model</strong><span class="classifier">torch.nn.Module</span></dt><dd><p>The model containing PerformerAttention modules.</p>
</dd>
<dt><strong>redraw_interval</strong><span class="classifier">int or None, optional</span></dt><dd><p>Interval for redrawing random projections. If None, projections are not redrawn. Default is None.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.RedrawProjection.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">redraw_interval</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.RedrawProjection.__init__" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="topobench.nn.backbones.graph.gps.RedrawProjection.redraw_projections">
<span class="sig-name descname"><span class="pre">redraw_projections</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#topobench.nn.backbones.graph.gps.RedrawProjection.redraw_projections" title="Link to this definition">#</a></dt>
<dd><p>Redraw random projections in PerformerAttention modules if needed.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt>None</dt><dd><p>None.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="topobench.nn.backbones.graph.nsd_utils.sheaf_models.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">topobench.nn.backbones.graph.nsd_utils.sheaf_models module</p>
      </div>
    </a>
    <a class="right-next"
       href="topobench.nn.backbones.graph.graph_mlp.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">topobench.nn.backbones.graph.graph_mlp module</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.Any"><code class="docutils literal notranslate"><span class="pre">Any</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.GINConv"><code class="docutils literal notranslate"><span class="pre">GINConv</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.GINConv.__init__"><code class="docutils literal notranslate"><span class="pre">GINConv.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.GINConv.forward"><code class="docutils literal notranslate"><span class="pre">GINConv.forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.GINConv.message"><code class="docutils literal notranslate"><span class="pre">GINConv.message()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.GINConv.message_and_aggregate"><code class="docutils literal notranslate"><span class="pre">GINConv.message_and_aggregate()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.GINConv.reset_parameters"><code class="docutils literal notranslate"><span class="pre">GINConv.reset_parameters()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.GPSConv"><code class="docutils literal notranslate"><span class="pre">GPSConv</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.GPSConv.__init__"><code class="docutils literal notranslate"><span class="pre">GPSConv.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.GPSConv.forward"><code class="docutils literal notranslate"><span class="pre">GPSConv.forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.GPSConv.reset_parameters"><code class="docutils literal notranslate"><span class="pre">GPSConv.reset_parameters()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.GPSEncoder"><code class="docutils literal notranslate"><span class="pre">GPSEncoder</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.GPSEncoder.__init__"><code class="docutils literal notranslate"><span class="pre">GPSEncoder.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.GPSEncoder.forward"><code class="docutils literal notranslate"><span class="pre">GPSEncoder.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.PNAConv"><code class="docutils literal notranslate"><span class="pre">PNAConv</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.PNAConv.__init__"><code class="docutils literal notranslate"><span class="pre">PNAConv.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.PNAConv.forward"><code class="docutils literal notranslate"><span class="pre">PNAConv.forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.PNAConv.get_degree_histogram"><code class="docutils literal notranslate"><span class="pre">PNAConv.get_degree_histogram()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.PNAConv.message"><code class="docutils literal notranslate"><span class="pre">PNAConv.message()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.PNAConv.reset_parameters"><code class="docutils literal notranslate"><span class="pre">PNAConv.reset_parameters()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.PerformerAttention"><code class="docutils literal notranslate"><span class="pre">PerformerAttention</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.PerformerAttention.__init__"><code class="docutils literal notranslate"><span class="pre">PerformerAttention.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.PerformerAttention.forward"><code class="docutils literal notranslate"><span class="pre">PerformerAttention.forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.PerformerAttention.redraw_projection_matrix"><code class="docutils literal notranslate"><span class="pre">PerformerAttention.redraw_projection_matrix()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.RedrawProjection"><code class="docutils literal notranslate"><span class="pre">RedrawProjection</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.RedrawProjection.__init__"><code class="docutils literal notranslate"><span class="pre">RedrawProjection.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topobench.nn.backbones.graph.gps.RedrawProjection.redraw_projections"><code class="docutils literal notranslate"><span class="pre">RedrawProjection.redraw_projections()</span></code></a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2025, Topological-Intelligence Team, Inc..
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>